# -*- coding: utf-8 -*-
"""Project 2 - Medical costs Price Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_jGQzqc3I0T8I8z-YiinhNGTKt50z23r

# **Import the Depedencies**
*   pandas - making dataframes
*   matplotlib& seaborn - used to make graphs & plots
*   sklearn - to perform regression / classification
*   metrics - to identify outliers / errors etc.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""# **Data Collection and preprocessing**"""

df = pd.read_csv('/content/insurance.csv')
df.head()

df.shape

df.describe()

"""## **Handling Missing values**"""

df.isnull().sum()

"""## **Handling Duplicated Values**"""

Duplicate_Value = df.duplicated().sum()
print(f'Number of duplicated rows are = {Duplicate_Value}')

df = df.drop_duplicates()

print("Number of Duplicate values are =" ,df.duplicated().sum())

"""## **Distribution of charges**"""

sns.set(style='whitegrid')
f, ax = plt.subplots(1,1, figsize=(12, 8))
ax = sns.distplot(df['charges'], kde = True, color = 'c')
plt.title('Distribution of Charges')

"""## **Normalizing the data**"""

f, ax = plt.subplots(1, 1, figsize=(12, 8))
ax = sns.distplot(np.log10(df['charges']), kde = True, color = 'r' )

"""## **Region-wise distribution**"""

charges = df['charges'].groupby(df.region).sum().sort_values(ascending=True)

# Creating the bar plot
f, ax = plt.subplots(1, 1, figsize=(8, 6))
sns.barplot(x=charges.head(), y=charges.head().index, palette='Blues', ax=ax)

"""## **Changes due to features**"""

# **Here we will categorize based on sex**
f, ax = plt.subplots(1, 1, figsize=(12, 8))
ax = sns.barplot(x='region', y='charges', hue='sex', data=df, palette='cool')

# Here we will categorize based on smoking
f, ax = plt.subplots(1,1, figsize=(12,8))
ax = sns.barplot(x = 'region', y = 'charges',
                 hue='smoker', data=df, palette='Reds_r')

# Here we will be categorize based on having children
f, ax = plt.subplots(1, 1, figsize=(12, 8))
ax = sns.barplot(x='region', y='charges', hue='children', data=df, palette='Set1')

"""### **Conclusion**

#### People in the Southwest generally smoke more than people in the Northeast, but people in the Northeast have higher charges by gender than in the Southwest and Northwest overall. And people with children tend to have higher medical costs overall as well

## **Analyzing Data**
"""

# analyze the medical charges by age, bmi and children according to the smoking factor

# Creating lmplot for age vs. charges
plt.figure(figsize=(12, 4))
ax1 = sns.lmplot(x='age', y='charges', data=df, hue='smoker', palette='Set1')
plt.subplots_adjust(wspace=0.5)  # Adjusting horizontal space between subplots

# Creating lmplot for bmi vs. charges
plt.figure(figsize=(12, 4))
ax2 = sns.lmplot(x='bmi', y='charges', data=df, hue='smoker', palette='Set2')
plt.subplots_adjust(wspace=0.5)  # Adjusting horizontal space between subplots

# Creating lmplot for children vs. charges
plt.figure(figsize=(12, 4))
ax3 = sns.lmplot(x='children', y='charges', data=df, hue='smoker', palette='Set3')
plt.subplots_adjust(wspace=0.5)  # Adjusting horizontal space between subplots

plt.show()

"""### **Conclusion**

Smoking has the highest impact on medical costs, even though the costs are growing with age, bmi and children.

## **Categorical Data Conversion**
"""

df[['sex', 'smoker', 'region']] = df[['sex', 'smoker', 'region']].astype('category')
df.info()

"""## **LabelEncoder**"""

#Converting category labels into numerical using LabelEncoder
from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()
label.fit(df.sex.drop_duplicates())
df.sex = label.transform(df.sex)
label.fit(df.smoker.drop_duplicates())
df.smoker = label.transform(df.smoker)
label.fit(df.region.drop_duplicates())
df.region = label.transform(df.region)
df.dtypes

"""## **Checking for multicollinearity**"""

"""Values above 0.75 (or below -0.75) indicate strong correlation between variables.
These values are more likely to lead to multicollinearity issues in regression analysis."""
f, ax = plt.subplots(1, 1, figsize=(10, 10))
ax = sns.heatmap(df.corr(), annot=True, cmap='cool')

"""Smoking closely related to many features

# **Model Selection**

Select the best model which gives good score

## **Linear Regression**
"""

from sklearn.model_selection import train_test_split as holdout
from sklearn.linear_model import LinearRegression
from sklearn import metrics
x = df.drop(['charges'], axis = 1)
y = df['charges']
x_train, x_test, y_train, y_test = holdout(x, y, test_size=0.2, random_state=0)
Lin_reg = LinearRegression()
Lin_reg.fit(x_train, y_train)
print(Lin_reg.intercept_)
print(Lin_reg.coef_)
print(Lin_reg.score(x_test, y_test))

"""## **Ridge Regression**"""

from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=0.5)
Ridge.fit(x_train, y_train)
print(Ridge.intercept_)
print(Ridge.coef_)
print(Ridge.score(x_test, y_test))

"""## **Lasso Regression**"""

from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

# Scale the features
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Initialize and fit the Lasso model
lasso_model = Lasso(alpha=0.2, fit_intercept=True, max_iter=1000, tol=0.0001, random_state=None, selection='cyclic')
lasso_model.fit(x_train_scaled, y_train)

# Print the intercept, coefficients, and R^2 score
print("Intercept:", lasso_model.intercept_)
print("Coefficients:", lasso_model.coef_)
print("R^2 score:", lasso_model.score(x_test_scaled, y_test))

"""## **Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

x = df.drop(['charges'], axis=1)
y = df.charges

Rfr = RandomForestRegressor(n_estimators=100, criterion='squared_error', random_state=1, n_jobs=-1)
Rfr.fit(x_train, y_train)

x_train_pred = Rfr.predict(x_train)
x_test_pred = Rfr.predict(x_test)

print('MSE train data: %.3f, MSE test data: %.3f' %
      (mean_squared_error(y_train, x_train_pred),
       mean_squared_error(y_test, x_test_pred)))
print('R2 train data: %.3f, R2 test data: %.3f' %
      (r2_score(y_train, x_train_pred),
       r2_score(y_test, x_test_pred)))

plt.figure(figsize=(8,6))

plt.scatter(x_train_pred, x_train_pred - y_train,
          c = 'gray', marker = 'o', s = 35, alpha = 0.5,
          label = 'Train data')
plt.scatter(x_test_pred, x_test_pred - y_test,
          c = 'blue', marker = 'o', s = 35, alpha = 0.7,
          label = 'Test data')
plt.xlabel('Predicted values')
plt.ylabel('Actual values')
plt.legend(loc = 'upper right')
plt.hlines(y = 0, xmin = 0, xmax = 60000, lw = 2, color = 'red')

print('Feature importance ranking\n\n')
importances = Rfr.feature_importances_
std = np.std([tree.feature_importances_ for tree in Rfr.estimators_],axis=0)
indices = np.argsort(importances)[::-1]
variables = ['age', 'sex', 'bmi', 'children','smoker', 'region']
importance_list = []
for f in range(x.shape[1]):
    variable = variables[indices[f]]
    importance_list.append(variable)
    print("%d.%s(%f)" % (f + 1, variable, importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(importance_list, importances[indices],
       color="y", yerr=std[indices], align="center")

"""Let's drop the region and sex as they are insignificant

## **Polynomial Regression**
"""

from sklearn.preprocessing import PolynomialFeatures
x = df.drop(['charges', 'sex', 'region'], axis = 1)
y = df.charges
pol = PolynomialFeatures (degree = 2)
x_pol = pol.fit_transform(x)
x_train, x_test, y_train, y_test = holdout(x_pol, y, test_size=0.2, random_state=0)
Pol_reg = LinearRegression()
Pol_reg.fit(x_train, y_train)
y_train_pred = Pol_reg.predict(x_train)
y_test_pred = Pol_reg.predict(x_test)
print(Pol_reg.intercept_)
print(Pol_reg.coef_)
print(Pol_reg.score(x_test, y_test))

"""## **Conclusion**

Polynomial regression model gives the best score
"""

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_test_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_test_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))

##Predicting the charges
y_test_pred = Pol_reg.predict(x_test)
##Comparing the actual output values with the predicted values
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})
df

"""# **Project Conclusion**

To give a good model for the prediction, we followed:
*   Data Collection
*   Data Preprocessing
*   Feature Engineering
*   Model Selection
*   Model performance Evaluation

and Polynomial regression model has given better performance score
"""

